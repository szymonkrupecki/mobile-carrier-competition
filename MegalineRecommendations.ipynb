{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls  minutes  messages   mb_used  is_ultra\n",
      "0   40.0   311.90      83.0  19915.42         0\n",
      "1   85.0   516.75      56.0  22696.96         0\n",
      "2   77.0   467.66      86.0  21060.45         0\n",
      "3  106.0   745.53      81.0   8437.39         1\n",
      "4   66.0   418.74       1.0  14502.75         0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "None\n",
      "             calls      minutes     messages       mb_used     is_ultra\n",
      "count  3214.000000  3214.000000  3214.000000   3214.000000  3214.000000\n",
      "mean     63.038892   438.208787    38.281269  17207.673836     0.306472\n",
      "std      33.236368   234.569872    36.148326   7570.968246     0.461100\n",
      "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
      "25%      40.000000   274.575000     9.000000  12491.902500     0.000000\n",
      "50%      62.000000   430.600000    30.000000  16943.235000     0.000000\n",
      "75%      82.000000   571.927500    57.000000  21424.700000     1.000000\n",
      "max     244.000000  1632.060000   224.000000  49745.730000     1.000000\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('/datasets/users_behavior.csv')\n",
    "\n",
    "#data_path = '/datasets/users_behavior.csv'\n",
    "#users_behavior = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df.drop('is_ultra', axis=1)\n",
    "y = df['is_ultra']\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=12345)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy: 0.7200622083981337\n",
      "Random Forest accuracy: 0.7884914463452566\n",
      "Logistic Regression accuracy: 0.7107309486780715\n",
      "\n",
      "Best model: RandomForestClassifier()\n",
      "Best accuracy of validation set: 0.7884914463452566\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    print(f'{name} accuracy: {accuracy}')\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "\n",
    "print(f'\\nBest model: {best_model}')\n",
    "print(f'Best accuracy of validation set: {best_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy (max_depth=None): 0.71850699844479\n",
      "Decision Tree accuracy (max_depth=1): 0.7542768273716952\n",
      "Decision Tree accuracy (max_depth=5): 0.7791601866251944\n",
      "Decision Tree accuracy (max_depth=10): 0.776049766718507\n",
      "\n",
      "Best Decision Tree model: DecisionTreeClassifier(max_depth=5, random_state=54321)\n",
      "Best Decision Tree accuracy: 0.7791601866251944\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning for Decision Tree (using max_depth)\n",
    "best_model_dt = None\n",
    "best_accuracy_dt = 0\n",
    "\n",
    "for max_depth in [None, 1, 5, 10]:  # Try different depths\n",
    "    model_dt = DecisionTreeClassifier(max_depth=max_depth, random_state=54321)\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    y_pred_dt = model_dt.predict(X_valid)\n",
    "    accuracy_dt = accuracy_score(y_valid, y_pred_dt)\n",
    "    print(f'Decision Tree accuracy (max_depth={max_depth}): {accuracy_dt}')\n",
    "\n",
    "    if accuracy_dt > best_accuracy_dt:\n",
    "        best_accuracy_dt = accuracy_dt\n",
    "        best_model_dt = model_dt\n",
    "\n",
    "print(f'\\nBest Decision Tree model: {best_model_dt}')\n",
    "print(f'Best Decision Tree accuracy: {best_accuracy_dt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criterion:\n",
    "\n",
    "- Gini Impurity measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. Lower Gini impurity indicates a better split.\n",
    "\n",
    "- Entropy measures the reduction in entropy (uncertainty) achieved by splitting the dataset on a particular feature. Higher information gain indicates a better split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=2, min_samples_leaf=1): 0.713841368584759\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=2, min_samples_leaf=2): 0.7371695178849145\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=2, min_samples_leaf=4): 0.7247278382581649\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=5, min_samples_leaf=1): 0.7402799377916018\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=5, min_samples_leaf=2): 0.7371695178849145\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=5, min_samples_leaf=4): 0.7247278382581649\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=10, min_samples_leaf=1): 0.7278382581648523\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=10, min_samples_leaf=2): 0.7371695178849145\n",
      "Decision Tree accuracy (max_depth=None, criterion=gini, min_samples_split=10, min_samples_leaf=4): 0.7293934681181959\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=2, min_samples_leaf=1): 0.7278382581648523\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=2, min_samples_leaf=2): 0.7511664074650077\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=2, min_samples_leaf=4): 0.7402799377916018\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=5, min_samples_leaf=1): 0.7371695178849145\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=5, min_samples_leaf=2): 0.749611197511664\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=5, min_samples_leaf=4): 0.7402799377916018\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=10, min_samples_leaf=1): 0.7169517884914464\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=10, min_samples_leaf=2): 0.7278382581648523\n",
      "Decision Tree accuracy (max_depth=None, criterion=entropy, min_samples_split=10, min_samples_leaf=4): 0.7356143079315708\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=2, min_samples_leaf=1): 0.7791601866251944\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=2, min_samples_leaf=2): 0.7776049766718507\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=2, min_samples_leaf=4): 0.7807153965785381\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=5, min_samples_leaf=1): 0.7791601866251944\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=5, min_samples_leaf=2): 0.7776049766718507\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=5, min_samples_leaf=4): 0.7807153965785381\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=10, min_samples_leaf=1): 0.7791601866251944\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=10, min_samples_leaf=2): 0.7776049766718507\n",
      "Decision Tree accuracy (max_depth=5, criterion=gini, min_samples_split=10, min_samples_leaf=4): 0.7807153965785381\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=2, min_samples_leaf=1): 0.7791601866251944\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=2, min_samples_leaf=2): 0.7776049766718507\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=2, min_samples_leaf=4): 0.7807153965785381\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=5, min_samples_leaf=1): 0.7791601866251944\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=5, min_samples_leaf=2): 0.7776049766718507\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=5, min_samples_leaf=4): 0.7807153965785381\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=10, min_samples_leaf=1): 0.7791601866251944\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=10, min_samples_leaf=2): 0.7776049766718507\n",
      "Decision Tree accuracy (max_depth=5, criterion=entropy, min_samples_split=10, min_samples_leaf=4): 0.7807153965785381\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=2, min_samples_leaf=1): 0.7744945567651633\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=2, min_samples_leaf=2): 0.7682737169517885\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=2, min_samples_leaf=4): 0.7620528771384136\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=5, min_samples_leaf=1): 0.7776049766718507\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=5, min_samples_leaf=2): 0.7713841368584758\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=5, min_samples_leaf=4): 0.7620528771384136\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=10, min_samples_leaf=1): 0.776049766718507\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=10, min_samples_leaf=2): 0.7682737169517885\n",
      "Decision Tree accuracy (max_depth=10, criterion=gini, min_samples_split=10, min_samples_leaf=4): 0.76049766718507\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=2, min_samples_leaf=1): 0.7729393468118196\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=2, min_samples_leaf=2): 0.7713841368584758\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=2, min_samples_leaf=4): 0.7589424572317263\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=5, min_samples_leaf=1): 0.7729393468118196\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=5, min_samples_leaf=2): 0.7713841368584758\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=5, min_samples_leaf=4): 0.7589424572317263\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=10, min_samples_leaf=1): 0.7636080870917574\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=10, min_samples_leaf=2): 0.7636080870917574\n",
      "Decision Tree accuracy (max_depth=10, criterion=entropy, min_samples_split=10, min_samples_leaf=4): 0.7558320373250389\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=2, min_samples_leaf=1): 0.7465007776049767\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=2, min_samples_leaf=2): 0.7480559875583204\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=2, min_samples_leaf=4): 0.7325038880248833\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=5, min_samples_leaf=1): 0.749611197511664\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=5, min_samples_leaf=2): 0.7527216174183515\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=5, min_samples_leaf=4): 0.7325038880248833\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=10, min_samples_leaf=1): 0.7480559875583204\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=10, min_samples_leaf=2): 0.7465007776049767\n",
      "Decision Tree accuracy (max_depth=15, criterion=gini, min_samples_split=10, min_samples_leaf=4): 0.7356143079315708\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=2, min_samples_leaf=1): 0.7511664074650077\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=2, min_samples_leaf=2): 0.7558320373250389\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=2, min_samples_leaf=4): 0.7465007776049767\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=5, min_samples_leaf=1): 0.7589424572317263\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=5, min_samples_leaf=2): 0.7636080870917574\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=5, min_samples_leaf=4): 0.7465007776049767\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=10, min_samples_leaf=1): 0.7371695178849145\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=10, min_samples_leaf=2): 0.744945567651633\n",
      "Decision Tree accuracy (max_depth=15, criterion=entropy, min_samples_split=10, min_samples_leaf=4): 0.7433903576982893\n",
      "\n",
      "Best Decision Tree model is: DecisionTreeClassifier(max_depth=5, min_samples_leaf=4, random_state=12345)\n",
      "Best Decision Tree accuracy: 0.7807153965785381\n"
     ]
    }
   ],
   "source": [
    "best_model_dt = None\n",
    "best_accuracy_dt = 0\n",
    "\n",
    "for max_depth in [None, 5, 10, 15]:\n",
    "    for criterion in ['gini', 'entropy']:\n",
    "        for min_samples_split in [2, 5, 10]:\n",
    "            for min_samples_leaf in [1, 2, 4]:\n",
    "                model_dt = DecisionTreeClassifier(\n",
    "                    max_depth=max_depth, \n",
    "                    criterion=criterion, \n",
    "                    min_samples_split=min_samples_split, \n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=12345)  \n",
    "                model_dt.fit(X_train, y_train)\n",
    "                y_pred_dt = model_dt.predict(X_valid)\n",
    "                accuracy_dt = accuracy_score(y_valid, y_pred_dt)\n",
    "                print(f'Decision Tree accuracy (max_depth={max_depth}, criterion={criterion}, min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf}): {accuracy_dt}')\n",
    "\n",
    "                if accuracy_dt > best_accuracy_dt:\n",
    "                    best_accuracy_dt = accuracy_dt\n",
    "                    best_model_dt = model_dt\n",
    "\n",
    "print(f'\\nBest Decision Tree model is: {best_model_dt}')\n",
    "print(f'Best Decision Tree accuracy: {best_accuracy_dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy: 0.7200622083981337\n",
      "Random Forest accuracy: 0.7884914463452566\n",
      "Logistic Regression accuracy: 0.7107309486780715\n",
      "\n",
      "Best model: RandomForestClassifier()\n",
      "Best accuracy of validation set: 0.7884914463452566\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = accuracy_score(y_valid, y_pred)\n",
    "    print(f'{name} accuracy: {accuracy}')\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "\n",
    "print(f'\\nBest model: {best_model}')\n",
    "print(f'Best accuracy of validation set: {best_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the Random Forest model seems promising cause it has the highest accuracy on the validation set\n",
    "- it suggests that the Random Forest algorithm is better at capturing the complex relationships between user behavior and plan preference compared to the other two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test set: 0.7900466562986003\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'Accuracy of test set: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.6842923794712286\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "# Calculate majority class (0 or 1) in training set\n",
    "majority_class = y_train.mode()[0]\n",
    "\n",
    "# Predict majority class for all test examples\n",
    "y_pred_baseline = [majority_class] * len(y_test)\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f'Baseline accuracy: {baseline_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calls: 0.20046339993643733\n",
      "minutes: 0.2416439836455758\n",
      "messages: 0.20927234403125186\n",
      "mb_used: 0.348620272386735\n"
     ]
    }
   ],
   "source": [
    "importances = best_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f'{feature}: {importance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that **call duration**(minutes) and **data usage**(mb_used) are the strongest predictors of which plan a user might prefer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      actual  predicted\n",
      "2498       0          1\n",
      "1748       0          1\n",
      "1077       1          0\n",
      "791        1          0\n",
      "2557       0          1\n",
      "...      ...        ...\n",
      "186        0          1\n",
      "1763       1          0\n",
      "2401       0          1\n",
      "2928       1          0\n",
      "2313       0          1\n",
      "\n",
      "[135 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with predictions and actual values\n",
    "results = pd.DataFrame({'actual': y_test, 'predicted': y_test_pred})\n",
    "\n",
    "# Filter misclassified examples\n",
    "misclassified = results[results['actual'] != results['predicted']]\n",
    "\n",
    "# Examine misclassified examples\n",
    "print(misclassified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows 138 instances where the model's predictions (the predicted column) did not match the actual user plan choices (the actual column). This means the model misclassified these users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False positives (predicted 1 but actual is 0): \n",
    "the model predicted that the user would prefer the Ultra plan, but the user actually chose the Smart plan.  This could happen if the user's usage patterns (e.g., call duration, data) were similar to typical Ultra users, but they chose the cheaper Smart plan for some other reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- False negatives (predicted 0, but the actual number is 1): The model predicted that the user would prefer the Smart plan, but they actually chose the Ultra plan. This could happen if the user's usage was lower than typical Ultra users, but they still valued the additional features or peace of mind that the Ultra plan offered.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
